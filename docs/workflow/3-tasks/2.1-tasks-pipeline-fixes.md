<!-- Updated: 2025-01-15 -->
# Task List: Finance Pipeline Fixes

## Problem Statement

The current pipeline has critical issues:
1. **WSJ items never marked as processed** - Items are re-processed every run
2. **No domain status tracking** - Failed domains aren't automatically blocked
3. **No relevance validation** - Crawled content might not match WSJ topic

---

## Current Data Flow

```
WSJ RSS → wsj_items (DB) → JSONL export → Google News search
        → embedding ranking → URL resolve → crawl → wsj_crawl_results (DB)
```

**Key insight**: After the workflow ends, JSONL files are deleted. All persistent data must come from database tables.

---

## Tasks

### Task 1: Mark WSJ Items as Processed (from DB)

**Status**: [x] COMPLETED

**What**: After saving crawl results to `wsj_crawl_results`, query that table to find which WSJ items had successful crawls, then mark them as processed in `wsj_items`.

**Why JSONL won't work**: GitHub Actions artifacts are ephemeral. The data is already saved to `wsj_crawl_results` table - we should query it directly.

**Decided approach**: Option A - Add `--mark-processed-from-db` command to `wsj_ingest.py`

**Implementation**:
1. Add function `cmd_mark_processed_from_db()` to `scripts/wsj_ingest.py`
2. Add CLI option `--mark-processed-from-db`
3. Add workflow step to `.github/workflows/finance-pipeline.yml`

**Code to add in `wsj_ingest.py`**:
```python
def cmd_mark_processed_from_db() -> None:
    """Mark WSJ items as processed by querying wsj_crawl_results table."""
    supabase = get_supabase_client()

    # Query wsj_crawl_results for successful crawls
    response = supabase.table('wsj_crawl_results') \
        .select('wsj_item_id') \
        .eq('crawl_status', 'success') \
        .not_.is_('wsj_item_id', 'null') \
        .execute()

    wsj_ids = list(set(row['wsj_item_id'] for row in response.data))
    updated = mark_items_processed(supabase, wsj_ids)
    print(f"Marked {updated} items as processed.")
```

**Workflow step**:
```yaml
- name: Mark WSJ items as processed
  run: cd scripts && python wsj_ingest.py --mark-processed-from-db
```

---

### Task 2: Create wsj_domain_status Table

**Status**: [x] COMPLETED

**User created table with schema**:
```
wsj_domain_status
├── domain (text, PK)
├── status (text)           -- 'active', 'blocked'
├── failure_type (text)     -- crawl error type
├── fail_count (int4)
├── success_count (int4)
├── last_failure (timestamptz)
├── last_success (timestamptz)
├── block_reason (text)
├── created_at (timestamptz)
└── updated_at (timestamptz)
```

---

### Task 3: Update Domain Status After Crawl

**Status**: [x] COMPLETED

**What**: After crawling, aggregate results from `wsj_crawl_results` and upsert to `wsj_domain_status`.

**Implementation**:
1. Add function to aggregate domain stats from `wsj_crawl_results`
2. Upsert to `wsj_domain_status` with counts and timestamps
3. Auto-block logic: `fail_count > 5 AND success_rate < 20%`

**Files to modify**:
- `scripts/wsj_ingest.py` - Add `--update-domain-status` command
- `.github/workflows/finance-pipeline.yml` - Add workflow step

---

### Task 4: Query Blocked Domains from DB

**Status**: [x] COMPLETED

**What**: Update `wsj_to_google_news.py` to query blocked domains from DB.

**Decided approach**: Combined (keep JSON as manual fallback + query DB)
```python
blocked = load_blocked_domains_json()  # Manual overrides (paywalls)
blocked |= query_blocked_from_db()     # Auto-detected from crawl failures
```

**Files to modify**:
- `scripts/wsj_to_google_news.py` - Update `load_blocked_domains()` function

---

### Task 5: Add Relevance Check

**Status**: [x] COMPLETED

**What**: Verify crawled content matches WSJ topic using embedding similarity.

**Implementation**:
- Location: `scripts/crawl_ranked.py` after successful crawl
- Compare: WSJ (title + description) vs Crawled (first 800 chars)
- Model: `all-MiniLM-L6-v2` (same as embedding_rank.py)
- Output: `relevance_score` field added to each article
- Flag: `relevance_flag = 'low'` if score < 0.25

**Note**: `all-MiniLM-L6-v2` has 256 token limit (~1000 chars)

---

## Execution Order

1. [x] ~~Task 2: Create domain status table~~ DONE
2. [x] ~~Task 1: Mark processed from DB~~ DONE
3. [x] ~~Task 3: Update domain status after crawl~~ DONE
4. [x] ~~Task 4: Query blocked domains from DB~~ DONE
5. [x] ~~Task 5: Relevance check~~ DONE

**All tasks completed!**

---

## Notes

- JSONL files don't persist after GitHub Actions run
- All data persistence must use Supabase tables
- `blocked_domains.json` kept as manual override for known paywalls
