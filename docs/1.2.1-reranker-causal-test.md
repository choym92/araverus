<!-- Updated: 2026-02-26 -->
# Cross-Encoder Reranker Test for News Threading

## Overview

Current threading (`7_embed_and_thread.py`) uses bge-base-en-v1.5 cosine similarity with centroid matching. This captures topical similarity ("same story") but misses causal relations ("Fed raises rates" → "Markets drop"). This experiment tests whether cross-encoder rerankers can detect both relationship types.

**Notebook**: `notebooks/reranker_causal_test.ipynb`
**Data**: `notebooks/reranker_causal_labels.pkl` (2,427 labeled pairs, 333 articles)
**Parent doc**: `docs/1.2-news-threading.md`

---

## Two Tasks Being Evaluated

| Task | Question | Current capability |
|------|----------|--------------------|
| **same_story** | Are these articles about the same developing story? | Yes — centroid matching (cosine > 0.73) |
| **causal** | Does article A's event cause/lead to article B's event? | No — cosine can't distinguish "related" from "causal" |

---

## Experiment Design

### Data Collection (Cell 1-3)

- **Source**: 333 articles from last 7 days (`wsj_items` + `wsj_embeddings`)
- **Candidate pair generation** — three nets to catch different relationship types:
  1. **Entity/ticker overlap** (≥1 shared) → 790 pairs
  2. **Embedding neighbors** (bge-base top-10 per article) → +1,437 pairs
  3. **Random negatives** (~200 pairs, different category, no overlap) → +200 pairs
- **Total**: 2,427 candidate pairs
- **Rules**: time-ordered (A published before B), ≤3 days apart, deduplicated

### Ground Truth Labeling (Cell 4)

- **Model**: Gemini 2.5 Flash (thinking model)
- **Approach**: Batch labeling — 175 batches, ~14 pairs/batch, 18 articles/batch
- **Runtime**: ~103 minutes total (rate-limited, $1-2 cost)
- **Labels per pair**:
  - `same_story`: same / related / unrelated
  - `causal`: A_causes_B / B_causes_A / no / unclear
- **CoT**: Gemini provides `reasoning` before labels for quality

### Label Distribution

| same_story | count | causal | count |
|------------|-------|--------|-------|
| unrelated | 1,150 | no | 1,943 |
| related | 975 | A_causes_B | 291 |
| same | 302 | unclear | 160 |
| | | B_causes_A | 33 |

### Models Tested (Cell 6-9)

| Model | Type | Size | How loaded |
|-------|------|------|------------|
| BAAI/bge-base-en-v1.5 | Bi-encoder (baseline) | 109M | Cosine from precomputed embeddings |
| BAAI/bge-reranker-v2-m3 | Cross-encoder | 568M | `CrossEncoder()` + sigmoid |
| Qwen3-Reranker-0.6B | CausalLM | 600M | `AutoModelForCausalLM` + yes/no token prob |
| tomaarsen/Qwen3-Reranker-0.6B-seq-cls | CrossEncoder (seq-cls) | 600M | `CrossEncoder(trust_remote_code=True)` |

Qwen3 models tested with two input variants:
- **(a) Plain**: title + description only
- **(b) Enriched**: title + description + extracted entities + keywords

### Evaluation Metrics (Cell 10)

- AUC, F1 (optimal threshold), Precision@5%
- Evaluated on three task variants: same_story (strict), same_story_broad (same+related), causal
- Score distribution histograms + ROC curve overlays

---

## Go/No-Go Criteria

| Criterion | Threshold |
|-----------|-----------|
| same_story Precision@5% | +10-20% absolute gain over cosine baseline |
| Temporal violations | < 5% on causal edges |
| Runtime | Acceptable for 20-30 unmatched articles/day |

---

## Results

### Label Quality (Cell 5)

- **2,419 / 2,427 labeled** (8 missing = 0.3%)
- **Temporal violations: 0 / 290** (0.0%) — all A_causes_B pairs have A published before B
- **Cross-tab** confirms logical consistency: `unrelated + A_causes_B = 0` (no causal link between unrelated articles)
- Spot-check 20 samples: Gemini reasoning accurate across all categories

| same_story × causal | A_causes_B | B_causes_A | no | unclear |
|---------------------|-----------|-----------|------|---------|
| **same** | 194 | 20 | 79 | 8 |
| **related** | 96 | 12 | 710 | 152 |
| **unrelated** | 0 | 0 | 1,148 | 0 |

### Baseline: bge-base-en-v1.5 cosine (Cell 6)

| Task | AUC | F1 | Precision@5% |
|------|-----|-----|--------------|
| same_story (strict) | 0.8728 | 0.6198 | 0.8333 |
| same_story (broad) | 0.7671 | 0.7293 | 0.9917 |
| causal | 0.8081 | 0.4703 | 0.5833 |

**Score separation by label** (mean cosine):

| Label | Mean | Std |
|-------|------|-----|
| same | 0.701 | 0.091 |
| related | 0.605 | 0.062 |
| unrelated | 0.549 | 0.074 |
| A_causes_B | 0.672 | 0.075 |
| no | 0.576 | 0.083 |

**Takeaway**: Cosine separates same vs unrelated well (Δ=0.15), but related vs unrelated (Δ=0.056) and causal vs no (Δ=0.096) overlap heavily. Cross-encoders need to improve these margins.

### bge-reranker-v2-m3 (Cell 7)

- **Runtime**: 52.7s for 2,427 pairs
- **Score range**: [0.5000, 0.7311] — **collapsed distribution**
- **Verdict**: Score range too narrow to be useful. This model is trained for query→document relevance (ms-marco style), not document→document comparison. All scores cluster near 0.5 after sigmoid.

### Qwen3-Reranker-0.6B CausalLM (Cell 8)

- **Runtime**: Plain 229s, Enriched 822s (3.6× slower with enriched input)
- **Score range**: [0.0, 0.99] — full dynamic range
- **Loading**: `AutoModelForCausalLM` + yes/no token probability extraction (NOT `CrossEncoder` class)

| Variant | same_story AUC | same_story F1 | same_story P@5% | causal AUC | causal F1 | causal P@5% |
|---------|---------------|--------------|----------------|-----------|----------|------------|
| Plain | 0.9291 | 0.7465 | 0.9500 | 0.8551 | 0.5014 | 0.7333 |
| Enriched | **0.9425** | **0.7640** | **0.9583** | **0.8695** | **0.5181** | **0.7417** |

**Takeaway**: Best performer across all metrics. Enriched variant adds ~3pp AUC gain but 4× runtime cost.

### Qwen3-Reranker-0.6B-seq-cls (Cell 9)

- **Loading**: `CrossEncoder('tomaarsen/Qwen3-Reranker-0.6B-seq-cls', trust_remote_code=True)`
- **Score range**: Binary-like distribution (clusters near 0 and 1)

| Variant | same_story AUC | causal AUC |
|---------|---------------|-----------|
| Plain | 0.7429 | 0.6815 |
| Enriched | 0.7511 | 0.6902 |

**Verdict**: Worse than cosine baseline. The SequenceClassification conversion doesn't work well for document→document tasks. **Not viable.**

### Full Model Comparison

| Model | same_story AUC | same_story F1 | causal AUC | causal F1 | Runtime |
|-------|---------------|--------------|-----------|----------|---------|
| bge-base cosine (baseline) | 0.8728 | 0.6198 | 0.8081 | 0.4703 | <1s |
| bge-reranker-v2-m3 | collapsed | — | collapsed | — | 53s |
| Qwen3-Reranker (plain) | 0.9291 | 0.7465 | 0.8551 | 0.5014 | 229s |
| **Qwen3-Reranker (enriched)** | **0.9425** | **0.7640** | **0.8695** | **0.5181** | 822s |
| Qwen3-seq-cls (plain) | 0.7429 | — | 0.6815 | — | ~60s |
| Qwen3-seq-cls (enriched) | 0.7511 | — | 0.6902 | — | ~60s |

---

## Critical Analysis

### Concerns About Qwen3 Results

1. **Easy task**: Random negatives (8% of pairs) inflate AUC — any model can separate unrelated pairs
2. **Binary-like scores**: Qwen3 CausalLM outputs cluster near 0 or ~0.99, making threshold selection brittle
3. **Circular labels**: Gemini labels used as ground truth may share Qwen3's LLM biases — not truly independent evaluation
4. **Weak causal F1**: 0.52 F1 for causal detection means real-world precision would be low
5. **Runtime**: 822s for enriched on 2,427 pairs ≈ impractical for daily pipeline (20-30 articles → ~200 pairs)
6. **Enriched doesn't help much**: +1.3pp AUC for 4× runtime — not worth the complexity
7. **Article-vs-article ≠ article-vs-centroid**: Pipeline uses centroid matching, not pairwise — results may not transfer directly

### Verdict: No-Go for Qwen3 Integration

The threshold tuning analysis (below) shows that a simple constant change achieves most of the quality improvement Qwen3 would provide, without any additional model complexity or runtime cost.

---

## Pipeline Comparison (Cell 10.5)

### Current Threading vs Gemini Ground Truth

Compared 301 pairs currently in the same thread against Gemini labels:

| Metric | Value |
|--------|-------|
| Same-thread pairs in dataset | 301 |
| Gemini "same" | 161 (53%) |
| Gemini "related" | 116 (39% — contamination) |
| Gemini "unrelated" | 23 (8% — errors) |

**Interpretation**: Current pipeline groups 39% "related" articles (same topic, different specific event) into the same thread. This isn't "wrong" per se — users may still find these useful — but it dilutes thread coherence.

---

## Threshold Tuning

### Base Threshold Sweep (THREAD_BASE_THRESHOLD)

Current: `0.62`. Swept `0.50–0.85` using Gemini labels as ground truth.

| Threshold | Predicted | True same | Related (contam) | Unrelated (error) | Precision | Recall | Contamination | F1 |
|-----------|-----------|-----------|------------------|--------------------|-----------|---------|--------------|----|
| **0.62** (current) | 886 | 271 | 423 | 192 | 0.306 | 0.898 | **0.694** | 0.456 |
| 0.65 | 684 | 257 | 313 | 114 | 0.376 | 0.852 | 0.624 | 0.522 |
| 0.68 | 508 | 235 | 198 | 75 | 0.463 | 0.779 | 0.537 | 0.581 |
| **0.70** (best F1) | 404 | 218 | 131 | 55 | 0.540 | 0.722 | **0.460** | 0.617 |
| 0.72 | 310 | 195 | 78 | 37 | 0.629 | 0.646 | 0.371 | 0.638 |
| **0.73** (≤20% contam) | 261 | 180 | 54 | 27 | 0.690 | 0.596 | **0.310** | 0.640 |
| 0.75 | 204 | 155 | 34 | 15 | 0.760 | 0.514 | 0.240 | 0.613 |

### Recommendation

**Raise `THREAD_BASE_THRESHOLD` from 0.62 to 0.70–0.73.**

- **0.70**: Best F1 (0.617). Contamination drops from 69% → 46%. Recall still solid (72%).
- **0.73**: If thread purity is priority. Contamination → 31%, but recall drops to 60%.

Note: The recall drop is mitigated by the LLM grouping step — articles missed by centroid matching can still be grouped by Gemini in the next pipeline step.

### Full Pipeline Flow (After Tuning)

1. **Centroid matching** (cosine ≥ 0.73 + time/size penalties): Catches ~60% of true same-story pairs with 69% precision
2. **LLM grouping** (Gemini for unmatched articles): Catches remaining 40% missed by centroid matching
3. **Cross-validation** (pairwise cosine ≥ 0.60): Filters 37% of Gemini groups that don't pass embedding verification (recall 88%)
4. **Merge check**: Combines similar threads (>0.92)

### LLM_GROUP_MIN_SIMILARITY Sweep

Current: `0.42`. Swept `0.35–0.70` on Gemini-labeled pairs (same + related only). Mean cosine: unrelated=0.549, related=0.605, same=0.701.

| Threshold | Pass | Reject | Reject rate | Precision | Recall | Contamination |
|-----------|------|--------|-------------|-----------|--------|---------------|
| **0.42** (was) | 1,267 | 4 | 0.3% | 0.238 | 1.000 | 0.762 |
| 0.50 | 1,219 | 52 | 4.1% | 0.245 | 0.993 | 0.755 |
| 0.55 | 1,085 | 186 | 14.6% | 0.267 | 0.963 | 0.733 |
| **0.60** (applied) | 801 | 470 | 37.0% | 0.332 | 0.884 | 0.668 |
| **0.69** (best F1) | 252 | 1,019 | 80.2% | 0.671 | 0.561 | 0.329 |

**Applied: 0.60.** Rationale:
- At 0.42: only 0.3% rejected — cross-validation was a no-op
- At 0.60: 37% rejected, recall still 88% (only 35 true "same" pairs lost)
- 0.69 (best F1) too aggressive — rejects 80% of Gemini groups, defeating the purpose of LLM grouping

---

## Design Discussion

### Causal Relationship Patterns

Two patterns relevant for news:

1. **Transitive chain** (A→B→C): "Fed raises rates" → "Dollar strengthens" → "EM sell-off"
   - A→C (indirect) is noisy — intermediate step B may not occur
   - **Only 1-hop causal links are reliable** for labeling
   - Transitive inference is a pipeline-level concern, not a model-level one

2. **Multiple causes** (A→B ← D): "Tariffs" → "Markets drop" ← "Weak jobs data"
   - More common in news — one outcome, multiple contributing events
   - A and D aren't the same story, but connect through B
   - Useful for building richer thread context

### Why Pairwise Evaluation Is Sufficient

The current experiment evaluates pairwise: "given two articles, score their relationship." This is the right level for model evaluation because:

- **Threading** uses pairwise scores (article vs centroid, or article vs article)
- **Causal graphs** are collections of pairwise edges — A→B and B→C edges form a chain naturally
- Graph-level concerns (transitivity, multi-cause DAGs) are **pipeline integration** decisions, not model selection criteria

### What This Experiment Does NOT Test

- Causal chain transitivity (A→B→C ∴ A→C?)
- Multi-cause aggregation (how to combine A→B and D→B edges)
- Full candidate distribution (only 7-day window, entity/embedding pre-filtered pairs)
- Production latency under real pipeline conditions

These are follow-up experiments if cross-encoders show promise.

---

## Conclusion: Threshold Tuning > Cross-Encoder

**Cross-encoder (Qwen3) is No-Go.** The practical improvement doesn't justify the complexity:
- Qwen3 adds +7pp AUC over cosine, but binary score distribution makes thresholding fragile
- 229–822s runtime per batch is impractical for daily pipeline
- Circular evaluation (Gemini labels → Gemini-like model) inflates metrics

**Threshold tuning is Go.** Simple constant changes achieve most quality gains:
- `THREAD_BASE_THRESHOLD`: 0.62 → **0.73** (contamination 69% → 31%, precision 31% → 69%)
- `LLM_GROUP_MIN_SIMILARITY`: 0.42 → **0.60** (reject rate 0.3% → 37%, recall still 88%)
- Zero runtime cost, zero new dependencies, 2-line code change

### Applied Changes

1. `THREAD_BASE_THRESHOLD` = 0.73 in `7_embed_and_thread.py` (line 37)
2. `LLM_GROUP_MIN_SIMILARITY` = 0.60 in `7_embed_and_thread.py` (line 43)
3. Updated `docs/1.2-news-threading.md` with new thresholds and version history

### Future

- Monitor thread quality after threshold changes (first few pipeline runs)
- If causal detection needed later: fine-tune lightweight model on Gemini labels (not generic reranker)

---

## Key Files

| File | Purpose |
|------|---------|
| `notebooks/reranker_causal_test.ipynb` | Full experiment notebook (17 cells) |
| `notebooks/reranker_causal_labels.pkl` | Saved labels + pairs + model scores (2.2MB) |
| `scripts/7_embed_and_thread.py` | Current threading pipeline |
| `docs/1.2-news-threading.md` | Threading system design (v3.3) |
| `notebooks/embedding_ab_test.ipynb` | Prior A/B test (reused patterns) |
