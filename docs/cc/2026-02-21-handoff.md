<!-- Created: 2026-02-21 -->
# Session Handoff — Per-Reason Failure Tracking Implementation

## What Was Done

Implemented per-reason failure tracking in `wsj_domain_status` to distinguish between "site crawl failure" vs "wrong article" for smarter auto-blocking.

### Completed:
- ✅ Created migration SQL: `supabase/migrations/011_domain_fail_counts.sql` (adds `fail_counts JSONB`)
- ✅ Updated `scripts/crawl_ranked.py`: Added error normalization (`CRAWL_ERROR_MAP`, `normalize_crawl_error()`)
  - Now sets `crawl_error = "low relevance"` for embedding score < 0.25
  - Now sets `crawl_error = "llm rejected"` for LLM rejection
  - All crawl failures normalized to natural language keys
- ✅ Updated `scripts/wsj_ingest.py` → `cmd_update_domain_status()`:
  - Added `CONTENT_MISMATCH_REASONS` = `{"low relevance", "llm rejected"}`
  - Added `is_blockable_failure()` — excludes content mismatch from blocking
  - Added aggregation with per-reason `fail_counts` JSONB tracking
  - Changed auto-blocking: Only **blockable** failures count toward Wilson score (excludes low relevance + LLM rejected)
  - Removed legacy LLM fail_count blocking logic
- ✅ Updated docs:
  - `docs/schema.md`: Added `fail_counts JSONB` column, updated auto-block rules
  - `docs/1-news-backend.md`: Added failure taxonomy table with blockable/non-blockable distinction
- ✅ Lint passed: `npm run lint:py`, `npm run lint:secrets`

---

## Remaining Work (DRY RUN TEST)

**IN THIS ORDER:**

### Step 1: Run Migration SQL (Supabase Dashboard)
```sql
ALTER TABLE wsj_domain_status
  ADD COLUMN IF NOT EXISTS fail_counts JSONB DEFAULT '{}'::jsonb;
```
- Go to Supabase Dashboard → SQL Editor
- Paste + execute
- Confirm no errors

### Step 2: Run Domain Status Update
```bash
cd /Users/youngmincho/Project/araverus
python scripts/wsj_ingest.py --update-domain-status
```

**Verify output:**
- Should print "Updated N domains" + "Auto-blocked M domains"
- Check blockers list shows `fail_counts` breakdown (e.g., `content too short=5, paywall=2`)

### Step 3: Query Results in Supabase
```sql
SELECT domain, status, fail_counts, success_count, fail_count, wilson_score
FROM wsj_domain_status
WHERE status = 'active' OR status = 'blocked'
LIMIT 20;
```

**Expected patterns:**
- `npr.org`: should be **active** (lots of `low relevance`, few blockable failures)
- `threads.com`: should be **blocked** (lots of `content too short`, no good success)
- `foxnews.com`: should be re-evaluated based on blockable-only failures

### Step 4: (Optional) Test New Crawl Error Normalization
```bash
python scripts/crawl_ranked.py --delay 1 --from-db --update-db
```

Then check `wsj_crawl_results`:
```sql
SELECT resolved_domain, crawl_error, COUNT(*)
FROM wsj_crawl_results
WHERE crawl_error IS NOT NULL
GROUP BY resolved_domain, crawl_error
LIMIT 30;
```

Should see natural language keys like `content too short`, `paywall`, `low relevance`, `llm rejected`, etc.

---

## Key Files Changed

| File | Change | Lines |
|------|--------|-------|
| `supabase/migrations/011_domain_fail_counts.sql` | NEW: Add fail_counts JSONB | - |
| `scripts/crawl_ranked.py` | Added `CRAWL_ERROR_MAP` + `normalize_crawl_error()` | 39-74 |
| `scripts/crawl_ranked.py` | Set crawl_error="low relevance" | ~448 |
| `scripts/crawl_ranked.py` | Set crawl_error="llm rejected" | ~497 |
| `scripts/crawl_ranked.py` | Normalize skip_reason on fail | ~567 |
| `scripts/crawl_ranked.py` | Normalize exception on error | ~576 |
| `scripts/wsj_ingest.py` | Added error normalization functions + CONTENT_MISMATCH_REASONS | ~725-764 |
| `scripts/wsj_ingest.py` | Updated aggregation loop with fail_counts | ~815-852 |
| `scripts/wsj_ingest.py` | Updated blocking logic + upsert | ~859-915 |
| `docs/schema.md` | Added fail_counts column + auto-block rule update | ~186-191 |
| `docs/1-news-backend.md` | Added failure taxonomy table | ~244-264 |

---

## Key Decisions

**Decision:** Exclude `low relevance` and `llm rejected` from auto-blocking
- **Reason:** These are content mismatch (article selection error), not domain problem. npr.org shouldn't be blocked just because embedding picked wrong article.

**Decision:** Track as natural language keys in JSONB (not enum)
- **Reason:** Flexible, debuggable, easy to add new reasons without DB schema changes.

**Decision:** Keep `failure_type` TEXT column in wsj_domain_status
- **Reason:** Legacy compatibility, last error human-readable fallback.

---

## Gotchas / Important Notes

1. **Migration is idempotent** — `IF NOT EXISTS` safe to re-run
2. **normalize_crawl_error()** in both `crawl_ranked.py` AND `wsj_ingest.py` — they're separate (wsj_ingest uses historical map for legacy data)
3. **fail_counts is JSONB** — always a dict, default `{}`
4. **Wilson score calculation now uses blockable_total** — not all failures
5. **Old LLM fail_count logic removed** — if any code references it, will error (intentional breaking change)

---

## Success Criteria

- [ ] Migration runs without errors
- [ ] `--update-domain-status` completes, shows fail_counts breakdown
- [ ] npr.org remains **active** (not blocked)
- [ ] threads.com remains **blocked** (blockable fails still high)
- [ ] fail_counts populated correctly in Supabase
- [ ] New crawls use natural language error keys
- [ ] No lint errors

---

## Session 2: Google News Search Optimization (2026-02-21)

### What Was Done

**Branch:** `feature/phase0-wsj-preprocess` (not yet merged to main)

1. **Date window tightened** — `wsj_to_google_news.py`
   - Before: Q1-Q2 = 7-day window, Q3-Q4 = 5-day window (different per query position)
   - After: All queries = 3-day window (pubDate ±1 day)
   - Removed Python-side date revalidation (redundant with Google `after:/before:`)

2. **Expanded `-site:` exclusions** — `format_query_with_exclusions()`
   - Before: `-site:wsj.com` only
   - After: `-site:wsj.com -site:reuters.com -site:bloomberg.com -site:ft.com -site:nytimes.com -site:barrons.com`
   - Frees up slots in Google's 100-result limit for free articles

3. **Removed all hardcoded domain lists**
   - `SOURCE_NAME_TO_DOMAIN` mapping (12 entries) — removed, Google RSS always provides `source url` domain
   - `EXCLUDED_SOURCES` (1 entry: 富途牛牛) — removed, `is_non_english_source()` already catches this
   - `UNCRAWLABLE_DOMAINS` in `domain_utils.py` (11 entries) — removed, all already in DB `wsj_domain_status`
   - `threads.com` updated from `active` to `blocked` in DB

4. **`load_wsj_jsonl` simplified** — removed hardcoded 9-field dict reconstruction, now passes through all JSONL fields

5. **Instrumentation logging** — removed 80-char query truncation, full queries now logged

### Test Results (5 items with LLM queries)

| Metric | Before (7d + wsj.com only) | After (3d + 6 sites excluded) |
|--------|---------------------------|-------------------------------|
| Total candidates | 693 | **990** (+43%) |
| Avg per item | 138.6 | **198.0** |

Narrower date window + paywall exclusions = more free articles in Google's 100-result slots.

### Query Analysis Findings

- **Optimal query length: 5-7 words** (avg 37 added per query)
- **13+ words nearly useless** (avg 2-14 added)
- **LLM queries contributed 81.8%** of all candidates (Q2-Q4 vs Q1 title)
- **All 42 items**: LLM found more candidates than title query alone
- **Google News RSS hard limit: ~100 results per query** (153/168 queries hit this cap)

### Files Changed

| File | Change |
|------|--------|
| `scripts/wsj_to_google_news.py` | Date window, site exclusions, removed hardcoded lists, simplified load_wsj_jsonl |
| `scripts/domain_utils.py` | Removed `UNCRAWLABLE_DOMAINS` hardcoded list |
| `docs/1.1-news-google-search.md` | Full pipeline documentation (merged from cc/) |

---

## Remaining Work

- [ ] **Run migration** from Session 1 (011_domain_fail_counts.sql)
- [ ] **Full pipeline test** with today's WSJ RSS — compare embedding scores vs baseline
- [ ] **Merge to main** — both Phase 0 + search optimization changes
- [ ] **Gemini prompt tuning** — add "5-7 words, paraphrase" guidelines based on query analysis
- [ ] Backfill old crawl_results with normalized errors (optional)

---

## Questions / Notes for Next Session

- After dry run, consider: Should we backfill old crawl_results with normalized errors? (Optional cleanup)
- LLM fail_count column could be deprecated (legacy, no longer used)
- Test with a few real crawls to make sure error normalization works as expected

