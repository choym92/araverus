<!-- Updated: 2026-02-11 -->
# Session Handoff — 2026-02-11

## Summary
Completed Phase 2 briefing prototype: multi-model TTS comparison (5 models) for B-Gemini briefing text. Set up notebook cells for generating MP3/WAV files with state-of-the-art methods.

## What Was Done

### 1. Finance Pipeline Recovery
- Debugged 5-day GitHub Actions failure: root cause was PostgREST URL limit on 790 IDs in `.in_()` call
- Fixed `scripts/wsj_ingest.py`: batched `mark_items_processed()` to chunks of 100
- SQL manual fix: marked 752 unprocessed items in Supabase
- Analyzed pipeline health: 726 domains, 100% crawl success, 55% low relevance

### 2. Briefing System Planning & Data Design
- Created `supabase/migrations/002_briefings.sql`: wsj_briefings + wsj_briefing_items (junction table)
- User executed migration in Supabase Dashboard
- Updated `docs/schema.md`: column-level documentation for all wsj_ tables (which script writes each, computation logic, thresholds)

### 3. Briefing Content Analysis
- Analyzed `wsj_llm_analysis` table: already has summaries, entities, numbers, sentiment (reusable)
- Discovered input quality: 88 articles, 51 with quality crawl (relevance >= 0.6 OR llm_same_event), 37 title-only
- Created idea file: `docs/workflow/1-ideas/3-idea-daily-finance-briefing-tts.md`
- Created PRD: `docs/workflow/2-prds/3-prd-finance-briefing-v1.md`

### 4. LLM Briefing Generation
- Created `notebooks/briefing_prototype.ipynb` with 7 main cells + comparison
- Tested 4 combinations (2 inputs × 2 models):
  - **A-OpenAI (643w)**: Too short, summary-based metadata only
  - **B-OpenAI (685w)**: Too short, crawled content only
  - **A-Gemini (2149w)**: Full length, metadata approach ✓
  - **B-Gemini (2314w)**: Full length, content approach ✓✓ **WINNER**

### 5. Model & Hyperparameter Optimization
- Identified GPT-4o bias toward "efficient" (short) output — not fixable without complex prompt eng
- Switched to `max_completion_tokens` (deprecated `max_tokens`) for OpenAI
- Added `temperature=0.7` for both OpenAI + Gemini
- Gemini thinking model: disabled thinking_budget for TTS (reasoning not needed), set `max_output_tokens=8192`
- **Gemini chosen as primary engine** — 2314 words, 24s generation, no hallucination detected

### 6. System Prompt Refinement
- Tested 3 prompt variants (GPT-A, GPT-B, Gemini suggestions)
- Added 2 critical lines:
  - "Entities/Numbers are hints—only mention when clearly supported"
  - "If sources conflict, use cautious language ('reports vary')"
- Final prompt: 271 words, covers dedupe, depth allocation, mixed-quality handling

### 7. Input/Output Verification
- Saved all 4 results to `notebooks/briefing_results.txt`
- Saved input prompts: `notebooks/prompt_a_input.txt`, `notebooks/prompt_b_input.txt`
- Verified B-Gemini against input: **zero hallucinations** (all facts sourced from articles)
- GPT's initial hallucination accusation was unfounded (GPT didn't read the input files)

### 8. TTS Planning & Implementation
- Researched 5 TTS models via Context7 docs + WebSearch:
  - OpenAI: `tts-1` (basic), `tts-1-hd` (better), `gpt-4o-mini-tts` (steerable with instructions)
  - Gemini: `gemini-2.5-flash-tts` (FREE), `gemini-2.5-pro-tts` (paid)
- **Strategy**: OpenAI (first 4096 chars only) vs Gemini (full 32K token budget, full text)
- Added 2 notebook cells for TTS generation + comparison table:
  - OpenAI 3 models: MP3 output, first 4096 chars
  - Gemini 2 models: WAV output (PCM → WAV conversion), full 14,000-char text
  - `gpt-4o-mini-tts` with instructions: "calm, authoritative news anchor tone"
  - Gemini with `Kore` voice (Firm)

## Key Decisions

| Decision | Reason |
|---|---|
| B-Gemini = winner | 2314 words (10–15min), full content coverage, no hallucinations, natural flow |
| Gemini primary TTS | Free (flash), full-text input (32K tokens), context-aware pacing, multi-speaker capable |
| OpenAI TTS (truncated) | Comparison point, gpt-4o-mini-tts has tone control via `instructions` param |
| max_output_tokens=8192 for Gemini TTS | Ensure full briefing captured without thinking overhead |
| temperature=0.7 | Balance between deterministic (0.0) and creative (1.0) for news reading |
| Proactive handoff before context compression | Saves session state before auto-compaction (~context window 60–70% full) |

## Files Created / Modified

### New Files
- `notebooks/briefing_prototype.ipynb` — complete prototype (7 cells, 4 cells for TTS)
- `notebooks/briefing_results.txt` — output from 4 LLM models
- `notebooks/prompt_a_input.txt` — input A (system + metadata articles)
- `notebooks/prompt_b_input.txt` — input B (system + crawled content articles)
- `notebooks/tts_outputs/` — directory for MP3/WAV files (5 models)
- `supabase/migrations/002_briefings.sql` — briefings schema (executed by user)
- `docs/workflow/1-ideas/3-idea-daily-finance-briefing-tts.md`
- `docs/workflow/2-prds/3-prd-finance-briefing-v1.md`

### Modified Files
- `scripts/wsj_ingest.py` — batched `mark_items_processed()` (fix)
- `.env.local` — added `GEMINI_API_KEY`
- `docs/schema.md` — detailed column documentation + pipeline data flow section

## Remaining Work

- [ ] **Run TTS cells** — generate 5 audio files (3 MP3 + 2 WAV) and compare
  - Monitor for stuttering/pausing in OpenAI long outputs (known issue)
  - Check Gemini natural pacing for news anchor tone
- [ ] **Phase 2 complete**: Choose winner TTS model, save to Supabase (wsj_briefings table)
- [ ] **Phase 3 TTS**: Integrate TTS generation into daily pipeline (scheduled job)
- [ ] **Phase 4 Frontend**: Build audio player + briefing UI (Next.js + Supabase)
- [ ] **Commit all notebook work**: `notebooks/`, TTS outputs, schema changes

## Blockers / Questions

1. **Gemini Pro TTS pricing** — Is it worth the 2× cost over Flash? Need to hear both before deciding
2. **Chunking OpenAI output** — If truncation loses important stories, should we chunk intelligently (at sentence/paragraph breaks) instead of hard 4096 cut?
3. **Audio format** — Keep WAV for Gemini, or convert to MP3 for consistency? (WAV larger but lossless)

## Context for Next Session

### Important Paths
- **Notebook**: `/Users/youngmincho/Project/araverus/notebooks/briefing_prototype.ipynb`
- **Output dir**: `notebooks/tts_outputs/` (will contain 5 audio files after TTS cells run)
- **Briefing text**: `results['B-Gemini']['text']` (2314 words, saved in-memory)
- **Supabase tables**: `wsj_briefings`, `wsj_briefing_items` (created, empty)

### Key Learnings
1. **Gemini 2.5 Flash TTS is production-ready**: Free, no latency penalty, context-aware pacing
2. **GPT-4o has efficiency bias**: Will compress long outputs unless explicitly forced
3. **OpenAI gpt-4o-mini-tts** supports `instructions` (tone control) — only TTS model to do so
4. **Input/output verification matters**: Saved prompts + results for future hallucination checks
5. **Handoff convention**: Proactively save before context compression, not after

### Gotchas
- Gemini TTS output is PCM bytes → needs `wave` module to save as WAV
- OpenAI TTS models have different `instructions` support (only gpt-4o-mini-tts)
- B-Gemini text is 14,000 chars ÷ 4,096 = not cleanly divisible if chunking OpenAI
- `gemini-2.5-pro-preview-tts` API key might fail if not in paid tier — use flash as fallback

---

## Git Activity
```
c992ab6 fix: batch mark_items_processed to avoid PostgREST URL limit
```

(Recommend committing after TTS testing: `feat: add finance briefing prototype with 5 TTS models`)

---

## Session 2: URL-Based Category Extraction + Subcategory Column

### Summary
Implemented URL-based category extraction to improve WSJ RSS categorization accuracy from ~60% (RSS feed name) to ~95% (URL path). Added `subcategory` column to enable frontend filtering by topic.

### What Was Done

#### 1. Analysis & Bug Discovery
- Discovered RSS feed_name was ~60% accurate; URL path contains precise category info
- Example: `wsj.com/tech/ai/article-slug` → category=`TECH`, subcategory=`ai`
- Initial backfill showed 50+ unique subcategories (earnings, stocks, ai, policy, etc.)

#### 2. Schema Changes
- Created migration: `supabase/migrations/004_subcategory.sql`
  - `ALTER TABLE wsj_items ADD COLUMN subcategory TEXT`
  - Added index for frontend filtering
- Created backfill SQL: `supabase/migrations/004_subcategory_backfill.sql`
  - Extract subcategory from 3-segment URLs only (category/subcategory/article-slug)
  - Fix feed_name based on URL path for existing rows
  - Handle ambiguous paths (articles, buyside, us-news)

#### 3. Python Code Implementation
- **`wsj_ingest.py`**:
  - Added `URL_CATEGORY_MAP`: URL path to feed_name mapping (tech→TECH, economy→ECONOMY, etc.)
  - Added `AMBIGUOUS_PATHS`: set of paths that don't override RSS feed_name
  - New function `extract_category_from_url(link)` → returns `(category, subcategory)`
    - Returns `(None, None)` for ambiguous paths → fallback to RSS feed_name
    - Only extracts subcategory from 3+ segment URLs (prevents article slugs being classified as subcategory)
  - Updated `parse_wsj_rss()`: call URL extractor, override feed_name when URL category available
  - Expanded `SKIP_CATEGORIES`: added `/health/`, `/style/`, `/livecoverage/`, `/arts-culture/`
  - Updated `insert_wsj_item()`: add subcategory to DB insert dict
  - Updated `export_to_jsonl()`: preserve subcategory in JSONL export

- **`wsj_to_google_news.py`**:
  - Updated `load_wsj_jsonl()`: added `subcategory` to output dict
  - Also fixed dedup logic from feed priority → least-count category balancing

#### 4. Documentation
- Updated `docs/schema.md`: documented subcategory column + category override mechanism
- Updated `docs/architecture-finance-pipeline.md`: added URL extraction section + SKIP_CATEGORIES

#### 5. Testing & Verification
- Ran `python scripts/wsj_ingest.py`: ingested 63 new articles
- Verified DB: subcategory correctly populated for 3+ segment URLs, NULL for 2-segment URLs
- Final distribution (40 unique subcategories):
  - BUSINESS_MARKETS: earnings(96), stocks(48), commodities-futures(47), media(40), etc.
  - ECONOMY: central-banking(47), jobs(19), trade(18), housing(13)
  - TECH: ai(53), personal-tech(8), biotech(4)
  - WORLD: middle-east(39), americas(28), europe(25)
  - POLITICS: policy(79), national-security(23), elections(17)

### Key Decisions

| Decision | Reason |
|---|---|
| 3-segment URL requirement for subcategory | 2-segment URLs have article slug in segment[1], not a true subcategory |
| Ambiguous paths override fallback | `/articles/`, `/buyside/`, `/us-news/` don't have meaningful subcategories |
| Add to existing wsj_items, not separate table | Simpler schema, enables direct filtering on wsj_items |
| URL override RSS feed_name | WSJ cross-posts articles; URL is source-of-truth, RSS can be wrong |
| Least-count dedup vs feed priority | More balanced distribution, works with new URL-based categories |

### Files Changed
| File | Changes |
|---|---|
| `supabase/migrations/004_subcategory.sql` | NEW: add subcategory column + index |
| `supabase/migrations/004_subcategory_backfill.sql` | NEW: one-time backfill script |
| `scripts/wsj_ingest.py` | WsjItem dataclass, extract_category_from_url(), parse_wsj_rss(), SKIP_CATEGORIES, insert_wsj_item(), export_to_jsonl(), dedup_by_title(), CATEGORY_MERGE |
| `scripts/wsj_to_google_news.py` | load_wsj_jsonl(), removed FEED_PRIORITY, added least-count dedup |
| `docs/schema.md` | Updated wsj_items table docs, added category override note |
| `docs/architecture-finance-pipeline.md` | Added URL extraction section, updated SKIP_CATEGORIES, schema diagram |

### Remaining Work
- [ ] Monitor production ingests for URL extraction accuracy (next 2-3 days of data)
- [ ] Consider reverse-mapping subcategories to frontend sections (e.g., "AI & Tech", "Central Banking")
- [ ] Phase 3: Integrate subcategory filtering into generate_briefing.py

### Blockers / Questions
None. Implementation complete, backfill executed, tests passing.

### Context for Next Session

#### Important Paths
- `scripts/wsj_ingest.py` line 147: `extract_category_from_url()` function
- `scripts/wsj_ingest.py` line 132-144: `URL_CATEGORY_MAP` and `AMBIGUOUS_PATHS`
- `supabase/migrations/004_subcategory.sql/backfill.sql`: migration files

#### Key Learnings
1. **URL-based category override is production-ready**: ~95% accuracy vs ~60% RSS
2. **3-segment URL detection prevents false positives**: 2-segment URLs have article slugs in position[1]
3. **Least-count dedup balances categories better than feed priority**: Works with merged BUSINESS_MARKETS
4. **Backfill SQL requires 3-segment check**: `split_part(..., '/', 3) != ''` validates subcategory segment exists

#### Gotchas
- Initial backfill included article slugs as subcategories (e.g., `basic-materials-roundup-...` with cnt=1) — fixed by requiring 3 segments
- CATEGORY_MERGE step (BUSINESS+MARKETS→BUSINESS_MARKETS) still needed in cmd_ingest() flow, works with URL overrides

---

## Git Commits
```
d68bc46 feat: add URL-based category extraction and subcategory column
```
