<!-- Created: 2026-01-19 -->
# Content Quality & Low Relevance Fix - Implementation Steps

## Background

**Problem:** 55 out of 377 crawled articles have `relevance_flag = 'low'`, meaning the crawled content doesn't match the WSJ article topic. Some of these are garbage content (CSS code, repeated words, paywall messages).

**Current behavior:** First article that successfully crawls (>500 chars) is marked as "success", even if:
- Content is garbage (CSS/JS, repeated words, paywall placeholder)
- Content is about a completely different topic

**Goal:** Only mark as "success" if content is readable AND relevant to the WSJ topic. If not, try backup articles.

---

## Step-by-Step Implementation

### Step 1: Add `is_garbage_content()` function to `crawl_ranked.py`

**Location:** After line 61 (after existing imports and constants)

**What it does:**
- Takes crawled text as input
- Returns `(True, reason)` if garbage, `(False, None)` if ok

**Detection rules:**
1. **Repeated words:** If >90% of words are duplicates (e.g., "word word word...")
2. **CSS/JS code:** If text contains multiple CSS patterns like `mask-image:url`, `{display:`, `@media`
3. **Paywall indicators:** If text contains "meterActive", "meterExpired", "piano", "subscribe to continue"

### Step 2: Modify the success condition in `crawl_ranked.py`

**Location:** Lines 192-233 (the main crawl loop)

**Current logic (simplified):**
```python
for article in crawlable:
    result = await crawl_article(url)
    if result["success"] and result["markdown_length"] > 500:
        # SUCCESS! Stop trying more articles
        success = True
        break
```

**New logic:**
```python
for article in crawlable:
    result = await crawl_article(url)
    if result["success"] and result["markdown_length"] > 500:
        content = result["markdown"]

        # CHECK 1: Is it garbage?
        is_garbage, reason = is_garbage_content(content)
        if is_garbage:
            article["crawl_status"] = "garbage"
            article["crawl_error"] = reason
            save_to_db(article)  # Save as garbage
            continue  # TRY NEXT BACKUP

        # CHECK 2: Is it relevant?
        relevance = compute_relevance_score(wsj_text, content)
        if relevance < 0.25:
            article["crawl_status"] = "low_relevance"
            article["relevance_score"] = relevance
            save_to_db(article)  # Save as low_relevance
            continue  # TRY NEXT BACKUP

        # PASSED BOTH CHECKS - real success
        article["crawl_status"] = "success"
        article["relevance_score"] = relevance
        article["relevance_flag"] = "ok"
        success = True
        break
```

### Step 3: Update `cmd_mark_processed_from_db()` in `wsj_ingest.py`

**Location:** Lines 487-522

**Current logic:**
```python
# Mark WSJ items as processed if they have ANY successful crawl
response = supabase.table('wsj_crawl_results') \
    .select('wsj_item_id') \
    .eq('crawl_status', 'success') \
    .execute()
```

**New logic:**
```python
# Only mark as processed if crawl is success AND relevance is ok
response = supabase.table('wsj_crawl_results') \
    .select('wsj_item_id') \
    .eq('crawl_status', 'success') \
    .eq('relevance_flag', 'ok') \
    .execute()
```

### Step 4: Deploy and test

1. Commit and push changes
2. Run pipeline manually: `gh workflow run "Finance Pipeline"`
3. Monitor logs for new `garbage` and `low_relevance` statuses

### Step 5: Fix existing data (SQL in Supabase)

After confirming code works:

```sql
-- 1. Reclassify existing low-relevance as needing retry
UPDATE wsj_crawl_results
SET crawl_status = 'low_relevance'
WHERE crawl_status = 'success' AND relevance_flag = 'low';

-- 2. Unmark their WSJ items so they get retried
UPDATE wsj_items
SET processed = false, processed_at = NULL
WHERE id IN (
  SELECT DISTINCT wsj_item_id FROM wsj_crawl_results
  WHERE crawl_status = 'low_relevance'
);

-- 3. Reactivate backup articles for those WSJ items
UPDATE wsj_crawl_results
SET crawl_status = 'pending'
WHERE crawl_status = 'skipped'
  AND wsj_item_id IN (
    SELECT DISTINCT wsj_item_id FROM wsj_crawl_results
    WHERE crawl_status = 'low_relevance'
  );
```

### Step 6: Update domain status logic in `wsj_ingest.py`

**Location:** `cmd_update_domain_status()` (lines 561-644)

**Current logic:**
```python
# Only counts 'failed' as failures
fail_count = count where crawl_status = 'failed'
```

**New logic:**
```python
# Count garbage + low_relevance as failures too
fail_count = count where crawl_status IN ('failed', 'garbage', 'low_relevance')
```

This means domains that consistently return garbage/low-relevance content will be auto-blocked (if fail > 5 AND success_rate < 20%).

### Step 7: Re-run pipeline

The 55 items with low relevance will now:
1. Have their backup articles tried
2. Only succeed if a backup has good relevance
3. Stay unprocessed if no good backup exists
4. Bad domains will be tracked and eventually auto-blocked

---

## New `crawl_status` Values

| Status | Meaning | Next Action |
|--------|---------|-------------|
| `success` | Good content + good relevance | Done |
| `garbage` | CSS, repeated words, paywall | Try backup |
| `low_relevance` | Readable but wrong topic | Try backup |
| `failed` | Crawl error (timeout, 404, etc) | Try backup |
| `skipped` | Backup article, another succeeded | None |
| `pending` | Not yet crawled | Will be tried |

---

## Expected Outcome

Before: 377 success (322 ok + 55 low)
After: ~320-350 success (all ok), low-relevance retried with backups

Some items may have NO good backup - those stay unprocessed for future improvements (better search queries, more sources).
