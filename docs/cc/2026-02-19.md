<!-- Updated: 2026-02-19 -->
# Session Handoff — 2026-02-19

## What Was Done

**Refactored domain blocking: JSON file → DB-only**
- Removed `scripts/data/blocked_domains.json` (63 blocked domains)
- Consolidated blocking logic from 3 separate implementations into shared DB-only pattern
- Added one-time migration: `--seed-blocked-from-json` flag to `wsj_ingest.py`
- Updated 5 scripts to use DB (`wsj_domain_status`) instead of local JSON
- Added in-memory `run_blocked` set to `crawl_ranked.py` for same-run failure tracking
- Updated docs + linted all changes

**Commits:**
- `e1929ca` refactor: remove blocked_domains.json, DB-only domain blocking (7 files, -693 LOC)

## Key Decisions

- **DB as single source of truth**: `wsj_domain_status.status='blocked'` replaces JSON file entirely
- **In-memory tracking in crawl_ranked.py**: Failed domains during run stored in `run_blocked` set (initialized from DB), prevents repeated attempts within same execution
- **Caller-provided blocked_domains param**: `crawl_article()` no longer loads JSON; callers pass `blocked_domains` set from DB
- **No log_crawl_result() anymore**: Removed JSON persistence of domain test results (all tracking now DB-only)

## Files Changed

| File | Change |
|------|--------|
| `scripts/wsj_ingest.py` | Added `cmd_seed_blocked_from_json()` + CLI flag, added `--help` entry |
| `scripts/domain_utils.py` | Removed L41-47 JSON loading block, pure DB-only `load_blocked_domains()` |
| `scripts/wsj_to_google_news.py` | Deleted `_load_blocked_domains_from_json()`, `_load_blocked_domains_from_db()`, `load_blocked_domains()` wrappers; import from `domain_utils`; removed unused `os` import |
| `scripts/crawl_article.py` | Deleted `BLOCKED_DOMAINS_FILE`, `load_blocked_domains()`, `save_blocked_domains()`, `is_domain_blocked()`, `log_crawl_result()` (all JSON-related). Removed `log_result` param from `crawl_article()`. CLI now loads DB via `domain_utils` |
| `scripts/crawl_ranked.py` | Added `run_blocked = set(blocked_domains)` for in-session failure tracking; `run_blocked.add(domain)` on error/failed states |
| `scripts/data/blocked_domains.json` | DELETED (63 blocked domains need to be seeded via `--seed-blocked-from-json` first) |
| `docs/4-news-backend.md` | Updated L118 (domain_utils), L43 (wsj_ingest flags), L91-101 (crawl_article, crawl_ranked descriptions), updated date stamp |

## Remaining Work

- [ ] **Run `python scripts/wsj_ingest.py --seed-blocked-from-json` once** to migrate JSON blocked domains to DB (one-time operation, then JSON file is gone)
- [ ] Verify tomorrow's pipeline run (2026-02-20) works without JSON file
- [ ] **Optional: Reduce crawl failures (18/68 success, 50 failed)** — social media (fb, x, ig) still reaching ranked results; could filter in `wsj_to_google_news.py` `is_source_blocked()` or at ranking stage

## Blockers / Questions

**None critical.** Pipeline ran full Phase 1-5 successfully today (2026-02-19), but:
- Today's run didn't use new code (JSON file still existed)
- 183 crawl attempts skipped as "Domain blocked (DB)" ✓ Good signal — DB blocking works
- 50/68 WSJ items failed crawl (73% fail rate) — mostly social media (fb.com, x.com, instagram.com, linkedin.com not blocked but low quality)
  - Worth investigating: should `is_source_blocked()` block more aggressively? Or rank lower?

## Context for Next Session

**Architecture (now clean):**
```
wsj_ingest.py          ← reads/writes wsj_items table
  ↓ export
wsj_to_google_news.py  ← searches Google News, uses domain_utils.load_blocked_domains() for filtering
  ↓ ranking
embedding_rank.py      ← ranks by embedding similarity
  ↓ resolve
resolve_ranked.py      ← resolves Google News URLs
  ↓ crawl
crawl_ranked.py        ← crawls with caller-provided blocked_domains set + in-session run_blocked tracking
  ↓ post-process
wsj_ingest.py --update-domain-status  ← aggregates results, auto-blocks bad domains
```

**Key files:**
- `scripts/domain_utils.py` — **New hub** for blocked domain logic (load_blocked_domains, is_blocked_domain)
- `scripts/crawl_ranked.py` — **Smart blocklist:** seeded from DB, updated in-session on failures
- DB table: `wsj_domain_status` — domain quality tracking + blocking

**One-time tasks still needed:**
- Run `python scripts/wsj_ingest.py --seed-blocked-from-json` to migrate 63 blocked domains from JSON to DB
  - Then JSON file is truly gone (already deleted from git)
  - Safe to run multiple times (uses `on_conflict='domain'` upsert)

**Next improvements to consider:**
1. **Social media filtering:** `wsj_to_google_news.py` should skip fb/x/ig/linkedin earlier (high crawl failure rate)
2. **Domain quality heuristics:** Weighted ranking by domain success_rate is good, but LinkedIn + aggregators still too noisy
3. **Crawl timeouts:** 50 failures could be timeouts — consider increasing crawl timeout or early exit on repeat failures

---
## Git Activity (auto-logged)
```
e1929ca refactor: remove blocked_domains.json, DB-only domain blocking
c07feae fix: remove preferred domain logic, fix JSONB storage for briefings
```
