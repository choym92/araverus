<!-- Updated: 2026-02-18 -->
# Session Handoff — 2026-02-18

## What Was Done

### Backend Pipeline Stage 1 (RSS Ingest) Comprehensive Review

**Completed:**
1. Fixed documentation inconsistencies in `docs/4-news-backend.md`
   - Line counts corrected (wsj_ingest.py: 926→1071, llm_analysis.py: 264→280)
   - LLM model names corrected (GPT-4o-mini → Gemini 2.5 Flash everywhere)
   - Environment variables documentation updated (GEMINI_API_KEY, GOOGLE_APPLICATION_CREDENTIALS)
   - GitHub Actions secrets corrected (OPENAI_API_KEY removed)

2. Resolved NULL subcategory problem (1.2)
   - Added `personal-finance`, `science` to `URL_CATEGORY_MAP`
   - Implemented subcategory fallback: `feed_name.lower().replace('_', '-')` when URL has no 3+ segments
   - Created `backfill_subcategory.py` script
   - Ran backfill: 435 items updated → **0 NULL remaining** (was 28%)

3. Removed junk content permanently (1.3)
   - Extended `SKIP_CATEGORIES`: added `/buyside/`, `/sports/`, `/opinion/`
   - Backfill deleted **36 junk items**
   - All future ingest will skip these categories

4. Fixed URL hash duplicates (1.4)
   - Modified `generate_url_hash()` to strip query params before hashing
   - Prevents `?mod=rss_Technology` from creating false duplicates
   - Deleted 1 existing duplicate (Adani article title variant)

5. Prevented "Roundup: Market Talk" spam (1.9 expansion)
   - Added title-based filter in `parse_wsj_rss()`: `if 'Roundup: Market Talk' in title: continue`
   - Deleted **96 items** + **240 related crawl_results**
   - Stale pending crawls (2 items) also deleted

6. Backfilled missing LLM analysis (1.10)
   - Discovered: 359 of 360 missing were `relevance_flag='low'` (intentionally skipped)
   - Only 5 items needed backfill (flag='ok' with no analysis)
   - Backfill completed successfully

7. Python linting setup
   - Installed ruff in venv
   - Fixed 31 lint errors automatically (unused imports, empty f-strings)
   - Created `scripts/ruff.toml` with exceptions for E402 (load_dotenv pattern) and F841 (debug vars)
   - Updated `package.json` npm script to use venv ruff path

**Verified as Non-Issues:**
- 1.5: `BUSINESS` vs `BUSINESS_MARKETS` feed_name — URL mapping already unifies
- 1.6: Stale old articles — 10 from first run (Dec 2025), none since
- 1.11: >5 crawl results per item — max 10, normal from pipeline reruns
- 1.12: 39% unprocessed items — searched but crawl failed/low relevance, normal

## Key Decisions

| Decision | Reason |
|----------|--------|
| Query param stripping in URL hash | Prevents false duplicates from RSS tracking params |
| Fallback subcategory = feed_name.lower() | Eliminates NULL while preserving category consistency |
| Roundup: Market Talk hardcoded filter | These daily digest posts never have crawlable content; permanent waste |
| Skip low-relevance LLM analysis | Cost optimization: embedding score < 0.25 already indicates poor match |

## Files Created/Modified

**Created:**
- `scripts/backfill_subcategory.py` — backfill + junk deletion (run once, can delete after)
- `scripts/ruff.toml` — Python linter config

**Modified:**
- `scripts/wsj_ingest.py`
  - `generate_url_hash()`: strip query params
  - `URL_CATEGORY_MAP`: added `personal-finance`, `science`
  - `SKIP_CATEGORIES`: added `/buyside/`, `/sports/`, `/opinion/`
  - `parse_wsj_rss()`: added fallback subcategory logic + Roundup filter
- `docs/4-news-backend.md` — comprehensive updates (lines, models, env vars, Stage 1 details)
- `docs/workflow/4-todo/backend-data-cleanup.md` — Stage 1 completed + verified non-issues
- `package.json` — `lint:py` now uses venv ruff

## DB Changes

| Table | Change |
|-------|--------|
| `wsj_items` | -36 junk items, -1 duplicate, -96 Roundup articles = **-133 total** |
| `wsj_crawl_results` | -240 Roundup-related crawls, -2 stale pending = **-242 total** |
| **Total items now** | **1662 → 1566** |

## Remaining Work

**Stage 2–9 Backend Pipeline Review (One at a Time):**
- [ ] **Stage 2** — `wsj_to_google_news.py` (1078 lines) — Google News search, 4 queries/item, dual-phase
- [ ] **Stage 3** — `embedding_rank.py` (232 lines) — semantic ranking
- [ ] **Stage 4** — `resolve_ranked.py` (304 lines) — URL resolution
- [ ] **Stage 5** — `crawl_ranked.py` (597) + `crawl_article.py` (1394) — crawling + quality
- [ ] **Stage 6** — `llm_analysis.py` (280 lines) — Gemini verification (already reviewed earlier)
- [ ] **Stage 7** — Post-process (`--mark-processed-from-db`, `--update-domain-status`)
- [ ] **Stage 8** — `embed_and_thread.py` — embeddings + threading
- [ ] **Stage 9** — `generate_briefing.py` — briefing + TTS

**Optional Backfills (after all stages reviewed):**
- Re-run LLM analysis on all crawls with new prompt (150-250 words vs old 1-2 sentence)
- Re-generate embeddings with updated summary text source
- Re-thread articles with new embeddings

## Context for Next Session

### Code Patterns
- `parse_wsj_rss()` has title filter checks early (Opinion, Roundup) — add new skips there
- `extract_category_from_url()` returns `(category, subcategory)` tuple — if None, fallback to feed_name
- `generate_url_hash()` now uses `urlparse()` to clean URLs before hashing

### Environment
- Python: venv at `scripts/.venv/bin/activate`
- Ruff config: `scripts/ruff.toml` (E402, F841 ignored)
- Mac Mini pipeline: uses macOS Keychain + `load_env.sh` for secrets
- GitHub Actions: uses `GEMINI_API_KEY`, `SUPABASE_KEY` secrets (no OpenAI)

### Known Gotchas
- WSJ RSS sometimes includes 1-2 old articles on first ingest — expected, monitor future runs
- Roundup articles cluster around market hours (may have multiple per day)
- URL query params (`?mod=...`) used by WSJ to track feed source — now stripped in hash
- `relevance_flag` categorization: `ok` → LLM analysis, `low` → skip (intentional)

### Cleanup Possible
- `scripts/backfill_subcategory.py` — one-time use, can delete after verification
- Check if `Roundup: Market Talk` filter should extend to other daily digest patterns (if any found in Stage 2–9 review)

## Verification

All changes verified:
- `npm run lint:py` — all checks pass
- `npm run build` — should pass (no TS changes)
- DB counts: 1662 items (from 1699), junk removed, NULL subcategory at 0

---

## Next Session Plan

1. Continue Stage 2 review (`wsj_to_google_news.py`)
   - Check for data quality issues (query params, domain filtering, date range)
   - Look for missed edge cases (non-English, newsletters, blocked domains)
2. Apply same review methodology: identify issues → group by root cause → fix code + DB + doc
3. After all 9 stages: prioritize which backfills to run (LLM summary + embeddings likely)
