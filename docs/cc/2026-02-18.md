<!-- Updated: 2026-02-18 -->
# Session Handoff â€” 2026-02-18

## What Was Done

### Backend Pipeline Stage 1 (RSS Ingest) Comprehensive Review

**Completed:**
1. Fixed documentation inconsistencies in `docs/4-news-backend.md`
   - Line counts corrected (wsj_ingest.py: 926â†’1071, llm_analysis.py: 264â†’280)
   - LLM model names corrected (GPT-4o-mini â†’ Gemini 2.5 Flash everywhere)
   - Environment variables documentation updated (GEMINI_API_KEY, GOOGLE_APPLICATION_CREDENTIALS)
   - GitHub Actions secrets corrected (OPENAI_API_KEY removed)

2. Resolved NULL subcategory problem (1.2)
   - Added `personal-finance`, `science` to `URL_CATEGORY_MAP`
   - Implemented subcategory fallback: `feed_name.lower().replace('_', '-')` when URL has no 3+ segments
   - Created `backfill_subcategory.py` script
   - Ran backfill: 435 items updated â†’ **0 NULL remaining** (was 28%)

3. Removed junk content permanently (1.3)
   - Extended `SKIP_CATEGORIES`: added `/buyside/`, `/sports/`, `/opinion/`
   - Backfill deleted **36 junk items**
   - All future ingest will skip these categories

4. Fixed URL hash duplicates (1.4)
   - Modified `generate_url_hash()` to strip query params before hashing
   - Prevents `?mod=rss_Technology` from creating false duplicates
   - Deleted 1 existing duplicate (Adani article title variant)

5. Prevented "Roundup: Market Talk" spam (1.9 expansion)
   - Added title-based filter in `parse_wsj_rss()`: `if 'Roundup: Market Talk' in title: continue`
   - Deleted **96 items** + **240 related crawl_results**
   - Stale pending crawls (2 items) also deleted

6. Backfilled missing LLM analysis (1.10)
   - Discovered: 359 of 360 missing were `relevance_flag='low'` (intentionally skipped)
   - Only 5 items needed backfill (flag='ok' with no analysis)
   - Backfill completed successfully

7. Python linting setup
   - Installed ruff in venv
   - Fixed 31 lint errors automatically (unused imports, empty f-strings)
   - Created `scripts/ruff.toml` with exceptions for E402 (load_dotenv pattern) and F841 (debug vars)
   - Updated `package.json` npm script to use venv ruff path

**Verified as Non-Issues:**
- 1.5: `BUSINESS` vs `BUSINESS_MARKETS` feed_name â€” URL mapping already unifies
- 1.6: Stale old articles â€” 10 from first run (Dec 2025), none since
- 1.11: >5 crawl results per item â€” max 10, normal from pipeline reruns
- 1.12: 39% unprocessed items â€” searched but crawl failed/low relevance, normal

## Key Decisions

| Decision | Reason |
|----------|--------|
| Query param stripping in URL hash | Prevents false duplicates from RSS tracking params |
| Fallback subcategory = feed_name.lower() | Eliminates NULL while preserving category consistency |
| Roundup: Market Talk hardcoded filter | These daily digest posts never have crawlable content; permanent waste |
| Skip low-relevance LLM analysis | Cost optimization: embedding score < 0.25 already indicates poor match |

## Files Created/Modified

**Created:**
- `scripts/backfill_subcategory.py` â€” backfill + junk deletion (run once, can delete after)
- `scripts/ruff.toml` â€” Python linter config

**Modified:**
- `scripts/wsj_ingest.py`
  - `generate_url_hash()`: strip query params
  - `URL_CATEGORY_MAP`: added `personal-finance`, `science`
  - `SKIP_CATEGORIES`: added `/buyside/`, `/sports/`, `/opinion/`
  - `parse_wsj_rss()`: added fallback subcategory logic + Roundup filter
- `docs/4-news-backend.md` â€” comprehensive updates (lines, models, env vars, Stage 1 details)
- `docs/workflow/4-todo/backend-data-cleanup.md` â€” Stage 1 completed + verified non-issues
- `package.json` â€” `lint:py` now uses venv ruff

## DB Changes

| Table | Change |
|-------|--------|
| `wsj_items` | -36 junk items, -1 duplicate, -96 Roundup articles = **-133 total** |
| `wsj_crawl_results` | -240 Roundup-related crawls, -2 stale pending = **-242 total** |
| **Total items now** | **1662 â†’ 1566** |

## Remaining Work

**Stage 2â€“9 Backend Pipeline Review (One at a Time):**
- [ ] **Stage 2** â€” `wsj_to_google_news.py` (1078 lines) â€” Google News search, 4 queries/item, dual-phase
- [ ] **Stage 3** â€” `embedding_rank.py` (232 lines) â€” semantic ranking
- [ ] **Stage 4** â€” `resolve_ranked.py` (304 lines) â€” URL resolution
- [ ] **Stage 5** â€” `crawl_ranked.py` (597) + `crawl_article.py` (1394) â€” crawling + quality
- [ ] **Stage 6** â€” `llm_analysis.py` (280 lines) â€” Gemini verification (already reviewed earlier)
- [ ] **Stage 7** â€” Post-process (`--mark-processed-from-db`, `--update-domain-status`)
- [ ] **Stage 8** â€” `embed_and_thread.py` â€” embeddings + threading
- [ ] **Stage 9** â€” `generate_briefing.py` â€” briefing + TTS

**Optional Backfills (after all stages reviewed):**
- Re-run LLM analysis on all crawls with new prompt (150-250 words vs old 1-2 sentence)
- Re-generate embeddings with updated summary text source
- Re-thread articles with new embeddings

## Context for Next Session

### Code Patterns
- `parse_wsj_rss()` has title filter checks early (Opinion, Roundup) â€” add new skips there
- `extract_category_from_url()` returns `(category, subcategory)` tuple â€” if None, fallback to feed_name
- `generate_url_hash()` now uses `urlparse()` to clean URLs before hashing

### Environment
- Python: venv at `scripts/.venv/bin/activate`
- Ruff config: `scripts/ruff.toml` (E402, F841 ignored)
- Mac Mini pipeline: uses macOS Keychain + `load_env.sh` for secrets
- GitHub Actions: uses `GEMINI_API_KEY`, `SUPABASE_KEY` secrets (no OpenAI)

### Known Gotchas
- WSJ RSS sometimes includes 1-2 old articles on first ingest â€” expected, monitor future runs
- Roundup articles cluster around market hours (may have multiple per day)
- URL query params (`?mod=...`) used by WSJ to track feed source â€” now stripped in hash
- `relevance_flag` categorization: `ok` â†’ LLM analysis, `low` â†’ skip (intentional)

### Cleanup Possible
- `scripts/backfill_subcategory.py` â€” one-time use, can delete after verification
- Check if `Roundup: Market Talk` filter should extend to other daily digest patterns (if any found in Stage 2â€“9 review)

## Verification

All changes verified:
- `npm run lint:py` â€” all checks pass
- `npm run build` â€” should pass (no TS changes)
- DB counts: 1662 items (from 1699), junk removed, NULL subcategory at 0

---

## Next Session Plan

1. Continue Stage 2 review (`wsj_to_google_news.py`)
   - Check for data quality issues (query params, domain filtering, date range)
   - Look for missed edge cases (non-English, newsletters, blocked domains)
2. Apply same review methodology: identify issues â†’ group by root cause â†’ fix code + DB + doc
3. After all 9 stages: prioritize which backfills to run (LLM summary + embeddings likely)

---

# Frontend Building Session â€” 2026-02-18

> **Summary:** This session is a **continuation of the same date**. Previous session completed **backend pipeline audit (Stage 1)**. This session focused on **news page redesign (3-column layout + thread carousel)**.

## What Was Done

### News Page Redesign: 3-Column + Thread Carousel

**Completed:**
1. **Modified `news-service.ts`**
   - Added optional `since` parameter to `getNewsItems()` for time-based filtering

2. **Rewrote `ArticleCard.tsx`** (major changes)
   - Converted to `'use client'` component
   - Added framer-motion carousel with slide animations (0.25s duration)
   - Removed `compact` variant (right column now uses `standard` cards)
   - New props: `id`, `threadTimeline`, `threadTitle`
   - Carousel always starts at **latest article** (end of timeline)
   - Thread indicator at card bottom: `â—€ N/M â–¶ thread-name` with disabled boundaries
   - Only shows indicator if `threadTimeline.length > 1`

3. **Updated `page.tsx`** (layout restructuring)
   - 3-column layout is **primary** (always renders on "Today" tab)
   - **36h â†’ 24h cutoff** with fallback: if < 6 recent articles, backfill with older ones
   - Parallel fetch: briefings + today's items + all items for backfill + thread timelines
   - Card slicing: featured (0) | left (1-5) | center (briefing) | right (6-11) | below-fold (12+)
   - Removed: `ThreadSection` component, `groupByThread()`, `computeHeatScore()`, thread-grouped JSX block
   - All cards now receive thread props: `id`, `threadTimeline`, `threadTitle`

4. **Deleted `ThreadSection.tsx`**
   - No longer needed; thread groups replaced with 3-column + carousel

5. **Verification**
   - `npm run lint` â€” no new errors (existing BriefingPlayer warnings only)
   - `npm run build` â€” success, `/news` compiles to 6.55kB

### Issue Resolution & Iteration

**Problem 1: Cache corruption**
- Dev server threw ENOENT errors on `.next/` manifest files
- Solution: `rm -rf .next/` + restart dev server

**Problem 2: 36h cutoff too aggressive**
- Only 1 article in 36h window (pipeline hadn't run recently)
- DB check showed: 39 unprocessed on 2/18, 1 processed from 2/17
- Solution: **24h cutoff with fallback logic** â€” if < 6 articles, fetch without time filter

**Problem 3: Carousel starting position**
- Initially showed first article in thread, user wanted latest
- Solution: Changed `initialIndex` to `threadTimeline.length - 1`

## Key Decisions

| Decision | Reason |
|----------|--------|
| 3-column as primary layout | Better for today's content + featured hero; thread view deferred to carousel interactions |
| Thread carousel optional/secondary | Today's articles are priority; thread history is bonus UX for exploring related coverage |
| 24h cutoff (not 36h) with fallback | Balances "fresh today" with "enough articles to fill layout"; gracefully degrades |
| Carousel starts at latest | User naturally scrolls backward (â—€) to see older articles in thread |
| Remove `compact` variant | Right column uses same `standard` cards; simplifies styling + consistency |

## Files Changed

**Major rewrites:**
- `src/app/news/_components/ArticleCard.tsx` â€” full client-side carousel implementation
- `src/app/news/page.tsx` â€” 3-column layout as primary, data fetching restructure

**Modifications:**
- `src/lib/news-service.ts` â€” added `since` parameter to `getNewsItems()`

**Deleted:**
- `src/app/news/_components/ThreadSection.tsx` â€” removed entirely

## Known Issues (DB-Level)

**Current state (2/18 14:46 UTC):**
- Briefings created successfully: 2/18 EN + KO both with audio_url + 72 sources
- But articles NOT processed: only 1/39 on 2/18 is `processed=true`
- Crawl status: 161 `ok`, 223 `low`, 616 `null` (incomplete pipeline run)

**Impact:** Page shows very few articles because query filters `processed=true + relevance_flag='ok'`. Pipeline likely ran RSS ingest + briefing generation but skipped crawl/LLM analysis stages.

**Possible solutions:**
1. Check Mac Mini cron/pipeline logs for which stages ran
2. Loosen DB query to include `low` or `null` crawl results (show RSS-only cards without summary/source)
3. Run missing pipeline stages manually

## Remaining Work

- [ ] **Confirm pipeline stages** â€” check Mac Mini logs to see which stages (crawl, LLM, thread, etc.) ran
- [ ] **Decide on crawl-missing articles** â€” show them with basic RSS data? or require full crawl?
- [ ] **Add Google News RSS fallback** (optional, user mentioned) â€” if crawl fails, show uncrawlable items from RSS to user
- [ ] **Test carousel interactions** â€” verify slide direction, boundary states, thread title truncation
- [ ] **Verify thread timeline ordering** â€” ensure timeline is sorted ascending by `published_at`
- [ ] **Edge cases:** articles without images, without summaries, without slug (external links)

## Blockers / Questions

1. **Pipeline execution:** Why did briefing gen complete but article processing stage not finish?
2. **Design decision:** Should cards show even if crawl/LLM stages failed (RSS-only view)?
3. **Google News RSS:** How to integrate non-crawlable sources? (user mentioned 2+ RSS feeds as fallback)

## Context for Next Session

### Frontend Code Patterns
- `ArticleCard` is now client component; carousel logic lives there
- `threadPropsFor()` helper in page.tsx builds `{id, threadTimeline, threadTitle}` for each card
- Thread timeline always sorted ascending by `published_at` (oldest â†’ newest)

### Data Flow
```
page.tsx:
  â†’ fetch today's items (24h)
  â†’ fetch all items (fallback if < 6 recent)
  â†’ fetch thread timelines in parallel
  â†’ build threadTimelines Map<threadId, NewsItem[]>
  â†’ render 3-column layout with carousel props
```

### UI Behavior
- Carousel: can only navigate if `threadTimeline.length > 1`
- Buttons disabled at boundaries (first/last article)
- Animation: slide 0.25s, 60px offset
- Indicator format: `â—€ 19/19 â–¶ Thread Title ðŸ“°`

### Known Gotchas
- **Thread timeline is oldest â†’ newest** (asc by published_at); carousel starts at `length-1` (latest)
- **Carousel state is local only** â€” refreshing page resets position to latest
- **No scroll restoration** â€” moving between threads doesn't remember carousel position in each
- **Preview images:** Some articles missing `top_image`; cards render without image section

### Environment
- Turbopack dev server on localhost:3000
- `.next` cache can corrupt from concurrent writes â€” `rm -rf .next/` + restart fixes
- `NEXT_PUBLIC_SUPABASE_URL` + `SUPABASE_SERVICE_ROLE_KEY` required for DB access

### Cleanup Possible
- Remove `scoreDisplay` references if not used elsewhere (was removed from ArticleCard)
- Consider centralized `threadPropsFor()` utility if it grows
