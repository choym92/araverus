<!-- Created: 2026-02-25 -->
# News Scoring Pipeline & Tuning

Complete reference for the multi-stage scoring system that ranks and filters crawl candidates.

---

## 1. Scoring Pipeline Overview

```
┌─────────────────┐     ┌──────────────────────┐     ┌───────────────────┐     ┌──────────────────┐
│ Stage 1          │     │ Stage 2               │     │ Stage 3            │     │ Stage 4           │
│ Embedding Rank   │────▶│ Weighted Score Sort   │────▶│ Quality Gates      │────▶│ Domain Auto-Block │
│ (4_embedding_    │     │ (6_crawl_ranked.py)   │     │ (6_crawl_ranked +  │     │ (domain_utils.py) │
│  rank.py)        │     │                       │     │  crawl_article.py) │     │                   │
│                  │     │ emb×0.50              │     │ 1. Garbage detect  │     │ Wilson < 0.15     │
│ top_k=40         │     │ wilson×0.25           │     │ 2. Content quality │     │ avg_llm < 3.0     │
│ min_score=0.3    │     │ llm×0.25              │     │ 3. Embedding ≥0.25 │     │ (both need n≥5)   │
│ bge-base-en-v1.5 │     │                       │     │ 4. LLM verify      │     │                   │
└─────────────────┘     └──────────────────────┘     └───────────────────┘     └──────────────────┘
```

**Flow**: For each WSJ item, Stage 1 selects the top-k most similar Google News candidates. Stage 2 re-sorts them by a weighted composite score. Stage 3 tries each candidate in order until one passes all quality gates. Stage 4 runs asynchronously (via `domain_utils.py`) to auto-block domains with consistently poor outcomes.

---

## 2. Stage 1: Embedding Rank (`4_embedding_rank.py`)

- **Model**: `BAAI/bge-base-en-v1.5` (sentence-transformers)
- **Query**: WSJ title + description concatenated
- **Candidates**: Google News article titles + source name
- **Metric**: Cosine similarity (dot product on L2-normalized vectors)
- **Filters**: `--top-k 40` (max candidates per WSJ item), `--min-score 0.3` (minimum similarity)

Score interpretation:
- `≥ 0.5` — High similarity
- `≥ 0.4` — Medium similarity
- `< 0.4` — Low similarity (still kept if ≥ 0.3)

---

## 3. Stage 2: Weighted Score Sorting (`6_crawl_ranked.py`)

After loading pending candidates, they are re-sorted using domain quality history:

```
weighted_score = 0.50 × embedding_score
               + 0.25 × wilson_score
               + 0.25 × (avg_llm_score / 10)
```

**Default values** (when domain has insufficient data):
- `wilson_score` → `0.4` (when `total_attempts < 3` or NULL)
- `avg_llm_score` → `5.0` (when NULL)
- `embedding_score` → `0.5` (when NULL, rare)

**Data source**: `wsj_domain_status` table, fetched once at script start. Only `status='active'` domains included.

**Rationale**: Embedding score captures content similarity; Wilson score captures domain reliability (success rate with confidence interval); LLM score captures content quality history.

---

## 4. Stage 3: Quality Gates (`6_crawl_ranked.py` + `crawl_article.py`)

Each candidate is tried in weighted_score order. All four gates must pass:

### 4.1 Garbage Detection (`is_garbage_content()`)

| Check | Threshold | Condition |
|-------|-----------|-----------|
| Empty content | — | `text` is falsy |
| Repeated words | unique_ratio < 0.1 | When `len(words) > 50` |
| CSS/JS code | css_matches ≥ 3 | Checks first 2000 chars for 6 patterns |
| Paywall | any match | 5 paywall indicator strings in first 1000 chars |
| Copyright/unavailable | any match | 9 unavailability strings in first 1000 chars |

### 4.2 Content Quality (`crawl_article.py` → `compute_quality_metrics()`)

| Metric | Fail Threshold | Reason Code |
|--------|---------------|-------------|
| Character length | < 350 chars | `TOO_SHORT` |
| Word count | < 60 words | `TOO_SHORT` |
| Character length | > 50,000 chars | `TOO_LONG` |
| Link line ratio | > 0.30 | `LINK_HEAVY` |
| Short line ratio | > 0.55 | `MENU_HEAVY` |
| Boilerplate ratio | > 0.40 | `BOILERPLATE_HEAVY` |

**Short-but-real fallback**: Articles ≥ 150 chars AND > 1.5× WSJ description length bypass `TOO_SHORT` but still must pass gates 4.1, 4.3, 4.4.

### 4.3 Embedding Relevance

- **Threshold**: cosine similarity ≥ 0.25
- **Comparison**: WSJ title+description vs first 800 characters of crawled content
- **Model**: Same `bge-base-en-v1.5` as Stage 1
- Below threshold → `relevance_flag='low'`, tries next candidate

### 4.4 LLM Verification

- **Model**: `gemini-2.5-flash-lite` (via `llm_analysis.py`)
- **Accept conditions**:
  - `is_same_event = true` → accept regardless of score
  - `is_same_event = false AND llm_score ≥ 7` → accept
- **Reject**: `is_same_event = false AND llm_score < 7` → `relevance_flag='low'`, tries next candidate

---

## 5. Stage 4: Domain Auto-Blocking (`domain_utils.py`)

Runs as part of `7_update_domain_status.py`. Blocks domains with consistently poor crawl outcomes:

| Metric | Threshold | Minimum Samples |
|--------|-----------|-----------------|
| Wilson score | < 0.15 | ≥ 5 attempts |
| Average LLM score | < 3.0 | ≥ 5 LLM scores |

Blocked domains are set to `status='blocked'` in `wsj_domain_status`. Manual blocks are preserved (not overwritten by auto-block logic).

---

## 6. Threshold Reference Table

Every hardcoded constant in the scoring pipeline:

| Threshold | Value | File:Line | Configurable? |
|-----------|-------|-----------|:---:|
| Embedding top_k | 40 | `4_embedding_rank.py:86` | Yes (`--top-k`) |
| Embedding min_score | 0.3 | `4_embedding_rank.py:87` | Yes (`--min-score`) |
| Weighted: emb/wilson/llm | 0.50/0.25/0.25 | `6_crawl_ranked.py:312` | No |
| Wilson default (unknown) | 0.4 | `6_crawl_ranked.py:310` | No |
| LLM default (unknown) | 5.0/10 | `6_crawl_ranked.py:311` | No |
| Domain min attempts for stats | 3 | `6_crawl_ranked.py:308` | No |
| Content min length | 350 ch / 60 words | `crawl_article.py:147` | No |
| Content max length | 50,000 ch | `crawl_article.py:149` | No |
| Link ratio max | 0.30 | `crawl_article.py:151` | No |
| Short line ratio max | 0.55 | `crawl_article.py:153` | No |
| Boilerplate ratio max | 0.40 | `crawl_article.py:155` | No |
| Short-but-real min | ≥ 150 ch, > 1.5× desc | `6_crawl_ranked.py:355` | No |
| Embedding relevance | ≥ 0.25 | `6_crawl_ranked.py:37` | No |
| Relevance compare chars | 800 | `6_crawl_ranked.py:38` | No |
| LLM accept (same event) | any score | `6_crawl_ranked.py:420` | No |
| LLM accept (diff event) | score ≥ 7 | `6_crawl_ranked.py:420` | No |
| Wilson block threshold | < 0.15 | `domain_utils.py:401` | No |
| Wilson min attempts | 5 | `domain_utils.py:402` | No |
| LLM quality block | < 3.0 | `domain_utils.py:422` | No |
| LLM quality min samples | 5 | `domain_utils.py:423` | No |
| Garbage: unique ratio | < 0.1 | `6_crawl_ranked.py:127` | No |
| Garbage: CSS match count | ≥ 3 | `6_crawl_ranked.py:134` | No |
| Garbage: word count threshold | > 50 | `6_crawl_ranked.py:125` | No |

---

## 7. Data Available for Analysis

What's currently stored in DB (`wsj_crawl_results`):

| Column | Type | Available Since | Notes |
|--------|------|-----------------|-------|
| `embedding_score` | FLOAT | Day 1 | From Stage 1 ranking |
| `relevance_score` | FLOAT | Day 1 | Post-crawl content similarity |
| `relevance_flag` | TEXT | Day 1 | `ok` or `low` — final decision |
| `llm_score` | INT | Day 1 | 0-10 from LLM analysis |
| `llm_same_event` | BOOLEAN | Day 1 | From LLM analysis |
| `crawl_status` | TEXT | Day 1 | `pending/success/failed/skipped/garbage` |
| `resolved_domain` | TEXT | Day 1 | For domain-level analysis |
| `weighted_score` | FLOAT | 2026-02-25 | Composite score used for sorting |
| `attempt_order` | INT | 2026-02-25 | 1-indexed rank within WSJ item |

---

## 8. Data Gaps Filled

Phase 2 of this plan adds two columns to `wsj_crawl_results`:

- **`weighted_score`** (FLOAT): The composite 50/25/25 score computed at crawl time. Stored for all candidates (including later-skipped ones) before the crawl loop begins.
- **`attempt_order`** (INT): 1-indexed position in the weighted-sorted candidate list. Rank 1 = tried first.

These enable:
- Comparing average weighted_score of `ok` vs `low` outcomes
- Measuring how often rank-1 candidates succeed (sorting effectiveness)
- Retrospective re-sorting with alternate weights

---

## 9. Future: Weight Optimization Approach

> **Not implemented.** Requires ~2 weeks of Phase 2 data accumulation.

### Retrospective Weight Simulation

Re-sort historical candidates with alternate weights, measure ok-at-rank-1 rate:
1. Query all `wsj_crawl_results` rows with `weighted_score IS NOT NULL`
2. Group by `wsj_item_id`
3. For each weight triple `(w_emb, w_wilson, w_llm)`:
   - Re-compute score per candidate
   - Re-sort
   - Check if rank-1 candidate has `relevance_flag='ok'`
4. Report ok-at-rank-1 rate per weight triple

### Grid Search Parameters

- `w_emb` ∈ [0.3, 0.4, 0.5, 0.6, 0.7]
- `w_wilson` ∈ [0.1, 0.15, 0.2, 0.25, 0.3]
- `w_llm` ∈ [0.1, 0.15, 0.2, 0.25, 0.3]
- Constraint: `w_emb + w_wilson + w_llm = 1.0`

### ROC Analysis

Use `weighted_score` as a binary classifier predictor:
- Positive label: `relevance_flag = 'ok'`
- Negative label: `relevance_flag = 'low'`
- Plot ROC curve, compute AUC
- Identify optimal threshold for score-based pre-filtering
