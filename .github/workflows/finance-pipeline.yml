# Finance TTS Briefing Pipeline
# Runs WSJ RSS ingestion → Google News search → Embedding ranking → URL resolution → Crawling
#
# Required Secrets:
#   SUPABASE_URL - Supabase project URL
#   SUPABASE_KEY - Supabase service role key
#   OPENAI_API_KEY - OpenAI API key (for LLM relevance check)

name: Finance Pipeline

on:
  workflow_dispatch:
    inputs:
      crawl_only:
        description: 'Skip search/rank/resolve, crawl pending from DB only'
        required: false
        default: 'false'
        type: boolean
      skip_crawl:
        description: 'Skip crawling step (faster test)'
        required: false
        default: 'false'
        type: boolean

  # Scheduled run (daily at 6 AM UTC = 3 PM KST)
  # schedule:
  #   - cron: '0 6 * * *'

env:
  NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
  SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_KEY }}
  PYTHON_VERSION: '3.11'
  UV_CACHE_DIR: /tmp/.uv-cache
  # HuggingFace settings
  TOKENIZERS_PARALLELISM: 'false'  # Suppress tokenizer warnings
  HF_HUB_DISABLE_TELEMETRY: '1'    # Disable telemetry

jobs:
  # ============================================================
  # Job 1: Ingest & Search (lightweight)
  # Skip if crawl_only=true
  # ============================================================
  ingest-search:
    if: ${{ inputs.crawl_only != true }}
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Set up uv (fast package manager)
        uses: astral-sh/setup-uv@v4
        with:
          enable-cache: true
          cache-dependency-glob: "scripts/requirements.txt"

      - name: Install dependencies (uv)
        run: uv pip install --system -r scripts/requirements.txt

      - name: Create output directory
        run: mkdir -p scripts/output

      - name: 1. WSJ RSS Ingest
        run: cd scripts && python wsj_ingest.py

      - name: 2. Export to JSONL (searched=false only)
        run: cd scripts && python wsj_ingest.py --export

      - name: 3. Google News Search
        run: cd scripts && python wsj_to_google_news.py --delay-item 0.5 --delay-query 0.3
        continue-on-error: true

      # NOTE: mark_items_searched moved to rank-resolve job (after DB save)

      - name: Upload search results
        uses: actions/upload-artifact@v4
        with:
          name: search-results
          path: |
            scripts/output/wsj_items.jsonl
            scripts/output/wsj_google_news_results.jsonl
          retention-days: 7

  # ============================================================
  # Job 2: Rank & Resolve (lightweight)
  # Skip if crawl_only=true
  # ============================================================
  rank-resolve:
    needs: ingest-search
    if: ${{ inputs.crawl_only != true }}
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Set up uv (fast package manager)
        uses: astral-sh/setup-uv@v4
        with:
          enable-cache: true
          cache-dependency-glob: "scripts/requirements.txt"

      - name: Install dependencies (uv)
        run: uv pip install --system -r scripts/requirements.txt

      - name: Cache HuggingFace models
        uses: actions/cache@v4
        with:
          path: ~/.cache/huggingface
          key: hf-models-${{ runner.os }}-all-MiniLM-L6-v2
          restore-keys: |
            hf-models-${{ runner.os }}-

      - name: Download search results
        uses: actions/download-artifact@v4
        with:
          name: search-results
          path: scripts/output

      - name: 4. Embedding Ranking
        run: cd scripts && python embedding_rank.py

      - name: 5. Resolve URLs & Save to DB
        run: cd scripts && python resolve_ranked.py --delay 0.5 --update-db
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}

      - name: 6. Mark items as searched (after DB save)
        run: cd scripts && python wsj_ingest.py --mark-searched output/wsj_items.jsonl

      - name: Upload ranked results
        uses: actions/upload-artifact@v4
        with:
          name: ranked-results
          path: scripts/output/wsj_ranked_results.jsonl
          retention-days: 7

  # ============================================================
  # Job 3: Crawl (heavy - Playwright)
  # When crawl_only=true: uses --from-db to crawl pending items
  # ============================================================
  crawl:
    needs: rank-resolve
    if: ${{ inputs.skip_crawl != true && inputs.crawl_only != true }}
    runs-on: ubuntu-latest
    timeout-minutes: 180

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Set up uv (fast package manager)
        uses: astral-sh/setup-uv@v4
        with:
          enable-cache: true
          cache-dependency-glob: "scripts/requirements.txt"

      - name: Install dependencies (uv)
        run: uv pip install --system -r scripts/requirements.txt

      # Playwright browser - always install fresh to avoid cache issues
      - name: Install Playwright browsers
        run: playwright install chromium --with-deps

      - name: Download ranked results
        uses: actions/download-artifact@v4
        with:
          name: ranked-results
          path: scripts/output

      - name: 7. Crawl Articles
        run: cd scripts && python crawl_ranked.py --delay 2 --update-db
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        continue-on-error: true

      - name: Upload crawl results
        uses: actions/upload-artifact@v4
        with:
          name: crawl-results
          path: |
            scripts/output/wsj_ranked_results.jsonl
            scripts/output/articles/
          retention-days: 7

  # ============================================================
  # Job 3b: Crawl Only (from DB) - when crawl_only=true
  # ============================================================
  crawl-from-db:
    if: ${{ inputs.crawl_only == true }}
    runs-on: ubuntu-latest
    timeout-minutes: 180

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Set up uv (fast package manager)
        uses: astral-sh/setup-uv@v4
        with:
          enable-cache: true
          cache-dependency-glob: "scripts/requirements.txt"

      - name: Install dependencies (uv)
        run: uv pip install --system -r scripts/requirements.txt

      # Playwright browser - always install fresh to avoid cache issues
      - name: Install Playwright browsers
        run: playwright install chromium --with-deps

      - name: Crawl pending items from DB
        run: cd scripts && python crawl_ranked.py --from-db --delay 2
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        continue-on-error: true

  # ============================================================
  # Job 4: Save to Supabase (persist results)
  # ============================================================
  save-results:
    needs: [rank-resolve, crawl]
    if: always() && needs.rank-resolve.result == 'success'
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Set up uv (fast package manager)
        uses: astral-sh/setup-uv@v4
        with:
          enable-cache: true
          cache-dependency-glob: "scripts/requirements.txt"

      - name: Install dependencies (uv)
        run: uv pip install --system -r scripts/requirements.txt

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: Merge artifacts
        run: |
          mkdir -p scripts/output
          cp -r artifacts/search-results/* scripts/output/ 2>/dev/null || true
          cp -r artifacts/ranked-results/* scripts/output/ 2>/dev/null || true
          cp -r artifacts/crawl-results/* scripts/output/ 2>/dev/null || true
          ls -la scripts/output/

      - name: Save to Supabase
        run: |
          cd scripts
          python -c "
          import json
          import os
          from datetime import datetime, timezone
          from pathlib import Path
          from supabase import create_client

          # Connect to Supabase
          url = os.environ['NEXT_PUBLIC_SUPABASE_URL']
          key = os.environ['SUPABASE_SERVICE_ROLE_KEY']
          supabase = create_client(url, key)

          # Load ranked results
          results_file = Path('output/wsj_ranked_results.jsonl')
          if not results_file.exists():
              print('No ranked results to save')
              exit(0)

          # Read and save
          saved = 0
          with open(results_file) as f:
              for line in f:
                  data = json.loads(line)
                  wsj = data.get('wsj', {})
                  ranked = data.get('ranked', [])

                  for article in ranked:
                      # Save all resolved articles with crawl status
                      if article.get('resolve_status') == 'success':
                          record = {
                              'wsj_item_id': wsj.get('id'),
                              'wsj_title': wsj.get('title'),
                              'wsj_link': wsj.get('link'),
                              'source': article.get('source'),
                              'title': article.get('title'),
                              'resolved_url': article.get('resolved_url'),
                              'resolved_domain': article.get('resolved_domain'),
                              'embedding_score': article.get('embedding_score'),
                              # Crawl data
                              'crawl_status': article.get('crawl_status'),
                              'crawl_error': article.get('crawl_error'),
                              'crawl_length': article.get('crawl_length'),
                              'content': article.get('crawl_markdown'),
                              'crawled_at': datetime.now(timezone.utc).isoformat() if article.get('crawl_status') == 'success' else None,
                              # Relevance check
                              'relevance_score': article.get('relevance_score'),
                              'relevance_flag': article.get('relevance_flag'),
                              # LLM analysis
                              'llm_same_event': article.get('llm_same_event'),
                              'llm_score': article.get('llm_score'),
                              # Metadata
                              'top_image': article.get('top_image'),
                          }
                          try:
                              supabase.table('wsj_crawl_results').upsert(
                                  record,
                                  on_conflict='resolved_url'
                              ).execute()
                              saved += 1
                          except Exception as e:
                              print(f'Error saving: {e}')

          print(f'Saved {saved} articles to Supabase')
          "

      - name: Mark WSJ items as processed
        run: cd scripts && python wsj_ingest.py --mark-processed-from-db

      - name: Update domain status
        run: cd scripts && python wsj_ingest.py --update-domain-status

      - name: Summary
        run: |
          echo "## Pipeline Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f scripts/output/wsj_ranked_results.jsonl ]; then
            TOTAL=$(wc -l < scripts/output/wsj_ranked_results.jsonl)
            echo "- WSJ items processed: $TOTAL" >> $GITHUB_STEP_SUMMARY
          fi

          echo "- Results saved to Supabase ✓" >> $GITHUB_STEP_SUMMARY

  # ============================================================
  # Job 4b: Post-process for crawl_only mode
  # ============================================================
  save-results-crawl-only:
    needs: crawl-from-db
    if: ${{ inputs.crawl_only == true }}
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Set up uv (fast package manager)
        uses: astral-sh/setup-uv@v4
        with:
          enable-cache: true
          cache-dependency-glob: "scripts/requirements.txt"

      - name: Install dependencies (uv)
        run: uv pip install --system -r scripts/requirements.txt

      - name: Mark WSJ items as processed
        run: cd scripts && python wsj_ingest.py --mark-processed-from-db

      - name: Update domain status
        run: cd scripts && python wsj_ingest.py --update-domain-status

      - name: Summary
        run: |
          echo "## Pipeline Results (Crawl Only Mode)" >> $GITHUB_STEP_SUMMARY
          echo "- Crawled pending items from database" >> $GITHUB_STEP_SUMMARY
          echo "- Results saved to Supabase ✓" >> $GITHUB_STEP_SUMMARY
